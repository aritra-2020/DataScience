#!pip install langchain
#!pip install langchain langchain-community
#!pip install langchain-ollama
#!pip install sentence-transformers faiss-cpu tiktoken python-dotenv -- HuggingFace‚Äôs library for loading and running LLMs.
#! pip install langchain_huggingface
##! pip install faiss-cpu
# Install required packages
!pip install langdetect
! pip install googletrans
#!pip install indicnlp
! pip install -U langchain-community
! pip install pypdf
! pip install -U langchain-huggingface
!pip install stanza
!pip install polyglot
!pip install pyicu pycld2 morfessor
!pip install deep_translator


# Auto-install FAISS if not already installed
try:
    import faiss
except ImportError:
    import subprocess, sys
    # Check if GPU is available
    try:
        import torch
        if torch.cuda.is_available():
            subprocess.check_call([sys.executable, "-m", "pip", "install", "faiss-gpu", "--quiet"])
            print("‚úÖ Installed faiss-gpu")
        else:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "faiss-cpu", "--quiet"])
            print("‚úÖ Installed faiss-cpu")
    except:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "faiss-cpu", "--quiet"])
        print("‚úÖ Installed faiss-cpu")

# Test import
import faiss
print(f"FAISS version: {faiss.__version__}")


import pkg_resources
print(pkg_resources.get_distribution("langchain").version)

# -------------------------------
# üîπ NLP + Utilities
# -------------------------------
import re
import os
import asyncio
import nest_asyncio
nest_asyncio.apply()

import pandas as pd
from langdetect import detect
from deep_translator import GoogleTranslator
from google.colab import files
from IPython.display import display, clear_output

# -------------------------------
# üîπ Stanza (Indian language sentence splitting)
# -------------------------------
import stanza

# -------------------------------
# üîπ Transformers (HuggingFace Models)
# -------------------------------
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    pipeline,
    MarianMTModel,
    MarianTokenizer
)

# -------------------------------
# üîπ LangChain (0.3.27 Compatible Imports)
# -------------------------------

# Document Loaders (community)
from langchain_community.document_loaders import (
    TextLoader,
    PyPDFLoader,
    CSVLoader
)

# Embeddings (community)
from langchain_community.embeddings import HuggingFaceEmbeddings

# Vectorstore (community)
from langchain_community.vectorstores import FAISS

# Prompt templates
from langchain_core.prompts import PromptTemplate, MessagesPlaceholder

# Runnables (LCEL)
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.runnables.history import RunnableWithMessageHistory

# Chat history (community)
from langchain_community.chat_message_histories import ChatMessageHistory

# Message types
from langchain_core.messages import HumanMessage, AIMessage

# Chains (new LC architecture)
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain


# üîπ Step 2: Imports
import stanza
import re
#import stanza
#stanza.download("bn", model_dir="./stanza_resources")
#from langchain.embeddings import HuggingFaceEmbeddings
#from langchain.vectorstores import FAISS
#from langchain.llms import HuggingFacePipeline
#from langchain.chains import RetrievalQA
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline , MarianMTModel, MarianTokenizer
from langchain.document_loaders import TextLoader, PyPDFLoader, CSVLoader
from langchain.text_splitter import CharacterTextSplitter #Splits documents into smaller chunks for embedding.
from langdetect import detect
from googletrans import Translator
#from langchain_huggingface import HuggingFacePipeline
from langchain_community.vectorstores import FAISS #Vector database for storing and retrieving document embeddings.
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import TextLoader, PyPDFLoader, CSVLoader
from google.colab import files
import pandas as pd
import os
import asyncio #Allows async/await so translations and queries can run efficiently.
from langchain_core.prompts import PromptTemplate
#from langchain.memory import ConversationBufferMemory # Corrected import - REMOVED
import requests
#from langdetect import detect
import ipywidgets as widgets
from IPython.display import display, clear_output
from langchain_core.prompts import PromptTemplate
#3from langchain.prompts import PromptTemplate
#from langchain.chains import LLMChain
#from langchain.memory import ConversationBufferMemory
#from langchain.chains import ConversationalRetrievalChain # REMOVED
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain.chains import create_history_aware_retriever
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
import nest_asyncio, asyncio
nest_asyncio.apply()
#import nest_asyncio, asyncio
#import ipywidgets as widgets
from IPython.display import display, clear_output
from deep_translator import GoogleTranslator
from langchain.schema.runnable import RunnableLambda
from langchain_core.prompts import MessagesPlaceholder




# üîπ Step 3: Upload and load document (PDF, CSV, or TXT)
uploaded = files.upload()
file_path = list(uploaded.keys())[0]
file_path

# üîπ Step 3: Upload and load document (PDF, CSV, or TXT)
if file_path.endswith(".pdf"):
    loader = PyPDFLoader(file_path)
elif file_path.endswith(".csv"):
    loader = CSVLoader(file_path)
else:
    loader = TextLoader(file_path, encoding='utf-8')

documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)
split_docs = text_splitter.split_documents(documents)

# Load LLM (Flan-T5 large)
"""Loads Flan-T5 Large model from HuggingFace.
   Wraps it in a LangChain-compatible HuggingFacePipeline."""

llm_model_id = "google/flan-t5-large"
#MODEL_NAME = "your-hf-model-name"  # e.g. "bert-base-uncased" or the HF model you're using
tokenizer = AutoTokenizer.from_pretrained(llm_model_id,use_fast=True)
model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_id)
pipe = pipeline("text2text-generation", model=model, tokenizer=tokenizer, max_new_tokens=256)
llm = HuggingFacePipeline(pipeline=pipe)

# Load FAISS retriever
"""Uses BAAI/bge-m3 embeddings (multilingual-capable).
    Creates a FAISS vector store from split_docs (your preprocessed dataset).
    Turns FAISS into a retriever for document search."""

embed_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3") #multilingual embeddings
#vectorstore = FAISS.load_local("faiss_index", embed_model, allow_dangerous_deserialization=True)
vectorstore = FAISS.from_documents(split_docs, embedding=embed_model)
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 2})

"""In the context of the error you saw
(AttributeError: 'coroutine' object has no attribute 'text'), the translator.translate method,
when called directly, was returning a coroutine object. A coroutine is a specific type of
awaitable object. You can't directly access attributes like .text on the coroutine itself
because it represents a future result, not the result right away. You need to await it first
to get the actual result which then would have the .text attribute"""

# Cell 6: Setup LangChain Conversational Chain with Memory
#memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)


# ============================
# 3. CHAT UI WIDGETS
# ============================
chat_box = widgets.Text(
    value='',
    placeholder='Ask about crops, climate, fertilizers...',
    description='üåæ Farmer:',
    layout={'width': '80%'}
)
output_box = widgets.Output()

# Translator setup -  Google translate client instance created
translator = Translator()

# -------------------------
#  GLOBAL SESSION STORE
# -------------------------
_session_store = {}

def get_session_history(session_id: str):
  if session_id not in _session_store:
    _session_store[session_id] = InMemoryChatMessageHistory()
  return _session_store[session_id]

def n_tokens(text: str) -> int:
  return len(tokenizer.encode(text, add_special_tokens=False))

def trim_text_by_tokens(text: str, allowed_tokens: int) -> str:
  tokens = tokenizer.encode(text, add_special_tokens=False)
  if len(tokens) <= allowed_tokens:
      return text
  # keep the last allowed_tokens tokens (you can keep first tokens instead)
  kept = tokens[-allowed_tokens:]
  return tokenizer.decode(kept, skip_special_tokens=True, clean_up_tokenization_spaces=True)

# Helper to translate synchronously
'''async def safe_translate(text, src_lang, dest_lang): # Made async
    result = translator.translate(text, src=src_lang, dest=dest_lang)
    return result.text  # always return plain string'''
'''def safe_translate(text, src_lang, dest_lang):
    try:
        result = translator.translate(text, src=src_lang, dest=dest_lang)
        return result.text
    except Exception as e:
        print(f"Translation error: {e}")
        return text'''

def translate_to_english(query: str) -> str:
    """
    Translate any language query into English using deep-translator.
    """
    return GoogleTranslator(source="auto", target="en").translate(query)

def translate_back_to_user_lang(answer: str, target_lang: str) -> str:
    """
    Translate the English answer back to the user's original language.

    Args:
        answer (str): The English output from LLM.
        target_lang (str): Target language code (e.g., 'hi', 'bn', 'ta', 'te').

    Returns:
        str: Translated text in the target language.
    """
    return GoogleTranslator(source="en", target=target_lang).translate(answer)

def lang_translator(text,SUPPORTED_STANZA,lang="en",output_style="bullets"):
      sentences = []
      # --- Try Stanza first ---
      if lang in SUPPORTED_STANZA:
          try:
              import stanza
              nlp = stanza.Pipeline(lang, model_dir="./stanza_resources", processors="tokenize", verbose=False)
              doc = nlp(text)
              sentences = [s.text for s in doc.sentences]
          except Exception as e:
              print(f"[Stanza failed ‚Üí {e}. Trying Polyglot]")

      # --- If no sentences yet, try Polyglot ---
      elif not sentences:
          try:
              from polyglot.text import Text
              poly_text = Text(text, hint_language_code=lang)
              sentences = [str(s) for s in poly_text.sentences]
          except Exception as e:
              print(f"[Polyglot failed ‚Üí {e}. Falling back to regex]")

      # --- Final fallback: regex split ---
      elif not sentences:
          sentences = re.split(r'[‡•§.?!]', text)

      # --- Clean up ---
      #print(f"sentences 1:{sentences}")

      sentences = [s.strip() for s in sentences if len(s.strip()) > 3]

      #print(f"sentences 2:{sentences}")

      # --- Format output ---
      if output_style == "bullets":
          return "\n".join([f"- {s}" for s in sentences[:5]])
      elif output_style == "numbered":
          return "\n".join([f"{i+1}. {s}" for i, s in enumerate(sentences[:5])])
      else:
          return text


# ============================
# 5. FOLLOW-UP QUESTION GENERATOR
# ============================
def suggest_questions(answer_en: str, user_lang: str, output_style: str, supported_lang: list): # Made async
    """Generate 3 follow-up questions from the Advisor's English response,
       then translate them back to the farmer's language."""
        #print("test 1...")
    try:
        #print("test 2...")
        suggestion_prompt = (
            f"Based on this farming answer:\n\n{answer_en}\n\n"
            "Suggest 3 short, specific follow-up farming questions the farmer might ask next. "
            "They should be about crops, soil, irrigation, fertilizers, or climate."
        )
        #print("test 3...")
        suggestions_en = llm.invoke(suggestion_prompt)
        suggestions_list = [s.strip("-‚Ä¢0123456789. ") for s in suggestions_en.split("\n") if s.strip()]
        suggestions_list = suggestions_list[:3]  # keep top 3
        #print("test 4...")
        # Translate each suggestion
        suggestions_translated = [
            #lang_translator(s,supported_lang,lang=user_lang,output_style=output_style) if user_lang != "en" else s
            #for s in suggestions_list
            translate_back_to_user_lang(s,user_lang) if user_lang != "en" else s for s in suggestions_list
        ]
        #print("test 5...")
        return suggestions_translated
    except Exception:
        return []

qa_prompt_template = PromptTemplate(
    input_variables=["input","context","tone","user_lang","style_instructions","chat_history"],
    #input_variables=["input","context","chat_history"],
    template="""
    You are an agricultural expert helping Indian farmers.
Answer the question based on the context retrieved.
The answer should be to the point and precise.
Avoid repeating facts in the response.
Use a {tone} tone.
Adhere to this answering format : {style_instructions}
Respond in {user_lang} language(same as user query).
Avoid hallucinating facts not present in the context.

Question:
{input}

Context:
{context}

Chat History:
{chat_history}

If the answer is not in the context, reply: "I could not find this information."
"""
)
#"chat_history"
#Chat History:
#{chat_history}

def process_query(user_query: str,tone: str,output_style: str,supported_lang): # Made async
    """Detects language, translates to English for QA, then back to user language.
       Also forces detailed answers if response is too short."""
    # Translate query to English
    #query_en = translator.translate(query_text, target="en", source=user_lang) if user_lang != "en" else query_text
     # Detect language
    user_lang = detect(user_query)
    print(f"Detected language: {user_lang}")
    #print("Inside process query before transalting input query...")
    #query_en = safe_translate(user_query, user_lang, "en") if user_lang != "en" else user_query # Await safe_translate
    #query_en = await translator.translate(user_query, src=user_lang, dest="en") if user_lang != "en" else user_query
    query_en = translate_to_english(user_query) if user_lang != "en" else user_query
    print(f"Translated query:{query_en}")


    # Retrieve docs in English
    """Uses FAISS retriever to get relevant documents based on the English query.
    Joins the page_content of all retrieved docs into a single context string."""
    docs = retriever.invoke(query_en,top_k=2)
    #print("Inside pq 1")
    #context = " ".join([doc.page_content for doc in docs])
    if not docs or all(len(doc.page_content.strip()) == 0 for doc in docs):
        not_found_message = "I could not find enough information in the provided data."
    #print("Inside pq 2")
    # Use the model's tokenizer (transformers) to count tokens and trim exactly to the model limit.
    #This ensures the total input remains <= model limit.
    # inside process_query:

    style_instructions = {
    "bullets": """- Format your answer STRICTLY as 1-2 bullet points.
- Each bullet point must be one sentence only.
- Do not write paragraphs.
- Do not repeat the context.
- Only include the most relevant facts for the question.""",

    "numbered": """- Format your answer STRICTLY as a numbered list (1-2 items).
- Each item must be one sentence only.
- Do not write paragraphs or extra text.""",

    "table": """- Format your answer STRICTLY as a markdown table.
- Use only 2‚Äì4 rows with concise entries.
- Do not add unrelated details.""",
}.get(output_style, """- Provide the answer in 1-2 concise sentences only.
- No repetition of the context.""")

    #from transformers import AutoTokenizer
    vars_dict = {
      "input": user_query,
      "context": "",
      "tone": tone,
      "user_lang": user_lang,
      "style_instructions": style_instructions,
      "chat_history": ""
      }
    prompt_prefix = qa_prompt_template.format(**vars_dict)
    '''prompt_prefix = qa_prompt_template.format(
        input=user_query,
        context="",
        tone=tone,
        user_lang=user_lang,
        style_instructions=style_instructions
        )'''

    prompt_tokens_est = n_tokens(prompt_prefix)
    MODEL_MAX_TOKENS = 512    # from the error
    ANSWER_TOKEN_BUDGET = 128
    allowed_context_tokens = MODEL_MAX_TOKENS - prompt_tokens_est - ANSWER_TOKEN_BUDGET
    if allowed_context_tokens < 50:
      allowed_context_tokens = 50

    context = " ".join([doc.page_content for doc in docs])
    trimmed_context = trim_text_by_tokens(context, allowed_context_tokens)
    '''max_context_chars = 1500
    trimmed_context = context[:max_context_chars]'''
    #vars_dict["context"] = trimmed_context
    #final_prompt = qa_prompt_template.format(**vars_dict)

    #####print(f"Trimmed context:{trimmed_context}")


    #print("Inside pq 3")

    print(">>> prompt tokens:", prompt_tokens_est)
    print(">>> context chars:", len(context))
    print(">>> allowed_context_tokens:", allowed_context_tokens)

    '''# Create LLMChain with your prompt
    qa_llm_chain = LLMChain(llm=llm, prompt=prompt_template)

    # ‚úÖ Instead of LLMChain, use prompt | llm -> Instead of using LLMChain, you should compose your prompt and LLM into a runnable using the new style:
    #This happens because in LangChain >0.1.17, ConversationalRetrievalChain internally expects the LLM chain to be Runnable (not LLMChain).

    # Build ConversationalRetrievalChain manually
    qa_chain = ConversationalRetrievalChain(
    retriever=retriever,
    combine_docs_chain=qa_llm_chain,
    memory=memory,
    return_source_documents=True  # optional
    )'''
    #print(f"Inside pq 4 - {final_prompt}")
    #print(f"Inside pq 4 - {type(final_prompt)}")
    # Create retriever - History-aware retriever
    history_aware_retriever = create_history_aware_retriever(
        llm=llm,
        retriever=retriever,
        prompt=qa_prompt_template
        #prompt = final_prompt
    )
    #print("Inside pq 5 -")
    # Documents chain - Stuff documents into answer chain
    doc_chain = create_stuff_documents_chain(llm,qa_prompt_template)
    #doc_chain = create_stuff_documents_chain(llm,final_prompt)
    #print("Inside pq 6 -")
    #Final QA chain
    qa_chain = create_retrieval_chain(
        history_aware_retriever,
        doc_chain
    )
    '''Please note qa_chain.invoke(chat_history:..) is for manually fetching and
    feeding the response and queries to memory, however  qa_chain_with_memory automatically takes care of it'''
    # ‚úÖ Wrap with memory - REMOVED: Manual memory management
    # qa_chain_with_memory = RunnableWithMessageHistory(
    #     qa_chain,
    #     lambda session_id: memory,   # use one memory object per session
    #     input_messages_key="input",
    #     history_messages_key="chat_history",
    #     output_messages_key="answer"
    # )

    # Call retrieval QA chain - Using the base qa_chain and passing history manually
    # response_en = qa_chain_with_memory.invoke( # REMOVED: Manual memory management
    # Store per-session chat histories
    session_id = "farmer1"


    qa_chain_with_history = RunnableWithMessageHistory(
     qa_chain,
    get_session_history ,   # <-- IMPORTANT: must return ChatMessageHistory, not ConversationBufferMemory
    input_messages_key="input",
    history_messages_key="chat_history",
    output_messages_key="answer"
    )
    '''call the chain asynchronously inside your async function'''


    '''def add_defaults(inputs):
      inputs["tone"] = inputs.get("tone", "formal")
      inputs["user_lang"] = inputs.get("user_lang", "en")
      inputs["style_instructions"] = inputs.get("style_instructions", "concise")
      return inputs

    qa_chain_with_defaults = RunnableLambda(add_defaults) | qa_chain_with_history

    response_en = qa_chain_with_defaults.invoke(
      {"input": query_en, "context": trimmed_context},
      config={"configurable": {"session_id": session_id}}
      )'''

    response_en = qa_chain_with_history.invoke( # MODIFIED: Using base chain
      {
      #"chat_history": formatted_history, # ADDED: Explicitly pass history
      "context": trimmed_context,
      "input": query_en,
      "tone": tone,
      "user_lang": user_lang,
      "style_instructions": style_instructions
      },
      config={"configurable": {"session_id": session_id}}
    )


    '''Your retriever uses history (via create_history_aware_retriever)
    Your memory keeps track of chat (RunnableWithMessageHistory)
    The UI loop stays the same, just call qa_chain_with_memory.invoke(...) instead of qa_chain.invoke(...)'''
    #print('The response is : ')
    #print(response_en["answer"])

    #response_en = qa_chain.invoke({"input": query_en, "chat_history": memory.chat_memory.messages})
    #print(response["answer"])   # Final answer

    # ‚úÖ Ensure response is meaningful (avoid "Yes"/"Helpful")
    if not response_en or len(response_en["answer"].strip()) < 15:
        response_en["answer"] = llm(
            f"Farmer asked: {query_en}\n\n"
            "The previous answer was too short. Give a clear and helpful agricultural explanation in 2‚Äì3 sentences."
        )

    '''# Normalize LLM output
    if isinstance(response_en, str):
        eng_answer_str = response_en
    elif hasattr(response_en, "content"):
        eng_answer_str = response_en.content
    elif isinstance(response_en, list) and len(response_en) > 0:
        eng_answer_str = response_en[0].text
    else:
        eng_answer_str = str(response_en)'''
    eng_answer_str = response_en["answer"]

    '''# Translate answer back to original language
    if user_lang != "en":
        final_answer = await safe_translate(eng_answer_str, "en", user_lang)
    else:
        final_answer = eng_answer_str'''

    #SUPPORTED_STANZA = ["hi", "ta", "te", "ur"]
    #query_lang_answer = lang_translator(eng_answer_str,supported_lang,lang=user_lang,output_style=output_style)
    query_lang_answer = translate_back_to_user_lang(eng_answer_str,user_lang) if user_lang != "en" else eng_answer_str
    #print(f" response :{eng_answer_str}")

    '''# üîπ Save latest Q&A into memory - ADDED: Manual memory update
    memory.save_context(
        {"input": user_query},    # in original user language
        {"output": query_lang_answer}  # answer in same language
    )'''

    return user_lang, user_query, query_lang_answer, eng_answer_str  # English kept for suggestion generator


# ============================
# 6. CHAT HANDLER
# ============================
#import asyncio # Import asyncio
def on_submit_handler(user_input,tone,op_style,supported_lang): # Made async
    #SUPPORTED_STANZA = ["hi", "ta", "te", "ur"]
    #supported_lang = SUPPORTED_STANZA
    #try:
    '''user_input = query.value.strip()
    query.value = ""'''
    #print("osa test 1...")
    #print(f"‚úÖ Entered on_submit_async with query: {user_input}")  # user_input is already a string
    if not user_input:
        return
    #print('osa test 2...')
    with output_box:
      #print('osa test 3...')
      clear_output(wait=True)
      #print("üöÄ on_submit_handler started")   # confirm entry
      print(f"User asked: {user_input}")
      #try:
      if not user_input:
        return

      # ‚úÖ Reset command
      if user_input.lower() in ["reset", "/reset"]:
          _session_store["farmer1"] = InMemoryChatMessageHistory()
          #get_session_history() = InMemoryChatMessageHistory()
          #memory.clear()
          print("‚úÖ Conversation history cleared")
          return

      #try:
      # Run multilingual process
      lang, original_q, answer, answer_en = process_query(user_input,tone,op_style,supported_lang) # Await process_query

      # ‚úÖ LangChain already stores history (no need to manually add Q/A)

      # Show conversation
      print("=== Conversation ===\n")
      prev_msg = None

      #chat_history = memory.load_memory_variables({}).get("chat_history", [])
      #chat_history = _session_store["farmer1"].messages if "farmer1" in _session_store else []

      #chat_history = _session_store["farmer1"].messages
      chat_history = get_session_history("farmer1").messages

      for msg in chat_history:
        #print("-- This is the msg content -- ")
        #print(msg)
        if msg.content != prev_msg:  # avoid duplicate display
          role = "üë§ Farmer" if msg.type == "human" else "ü§ñ Advisor"
          # Translate only if needed
          display_text = (
              translate_back_to_user_lang(msg.content,lang) if lang != "en" else msg.content
          )
          #print(f"msg content - {msg.content}")
          #print(f"lang - {lang}")
          #print(f"{role}: {msg.content}\n")
          print(f"{role}: {display_text}\n")
          prev_msg = msg.content

      # ‚úÖ Show follow-up suggestions
      followups = suggest_questions(answer, lang,op_style,supported_lang) # Await suggest_questions
      if followups:
          print("üí° Suggested Questions:")
          for f in followups:
            print(f"üëâ {f}")

      '''except Exception as e:
        #with output_box:
        print(f"‚ùå Error inside on_submit_async: {e}")
        #print(f"‚ùå Error: {str(e)}")
        #print("ü§ñ Please try again")'''

# ============================
# 7. CONNECT HANDLER TO CHATBOX
# ============================
SUPPORTED_LANGS = ["hi", "ta", "te", "ur"]
# Update on_submit handler to run the async function
#chat_box.on_submit(lambda query: asyncio.create_task(on_submit_async(query, "formal", 'bullets', SUPPORTED_LANGS))) # REMOVED asyncio.run
#chat_box.on_submit(lambda query: asyncio.run(on_submit(query, "formal", 'bullets', SUPPORTED_LANGS))) # ADDED asyncio.ensure_future
'''def on_submit(query):
    asyncio.ensure_future(on_submit_async(query, "formal", "bullets", SUPPORTED_LANGS))'''
# Sync wrapper for chatbox enter key
def on_submit(widget):
    query_text = widget.value.strip()
    widget.value = ""   # clear after hitting Enter
    #print("‚ö° on_submit called with:", query_text)
    if query_text:
      #loop = asyncio.get_event_loop()
      #loop.create_task(on_submit_async(query_text, "formal", "bullets", SUPPORTED_LANGS))
      #runner.run(on_submit_handler(query_text, "formal", "bullets", SUPPORTED_LANGS))  # ‚úÖ run async safely in background
      on_submit_handler(query_text, "formal", "bullets", SUPPORTED_LANGS)

chat_box.on_submit(on_submit)
#chat_box.observe(on_submit, names="value")
#chat_box.on_submit(lambda query: asyncio.ensure_future(on_submit_async(query, "formal", "bullets", SUPPORTED_LANGS)))
display(widgets.VBox([chat_box, output_box]))

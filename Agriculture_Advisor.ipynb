{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "defabadf445b4e04af5257f446e65557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36e77831eb804080a061ac41a91104a4",
              "IPY_MODEL_196602831903449d91f0b803b7170403"
            ],
            "layout": "IPY_MODEL_8e0883cf670741ceb7f84a4c38669006"
          }
        },
        "36e77831eb804080a061ac41a91104a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "ğŸŒ¾ Farmer:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_917d41b7d6f04e5da4fe66ff17606655",
            "placeholder": "Ask about crops, climate, fertilizers...",
            "style": "IPY_MODEL_ff40d37c81754933ab3a4655850b91b9",
            "value": ""
          }
        },
        "196602831903449d91f0b803b7170403": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_d7f66973ea904135a0c9279595f7bd3a",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "User asked: à¤§à¤¾à¤¨ à¤•à¥€ à¤–à¥‡à¤¤à¥€ à¤•à¥‡ à¤²à¤¿à¤ à¤‰à¤ªà¤¯à¥à¤•à¥à¤¤ à¤œà¤²à¤µà¤¾à¤¯à¥ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\n",
                  "Detected language: hi\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Translated query:What is the suitable climate for paddy cultivation?\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stderr",
                "text": [
                  "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  ">>> prompt tokens: 178\n",
                  ">>> context chars: 440\n",
                  ">>> allowed_context_tokens: 206\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "=== Conversation ===\n",
                  "\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "ğŸ‘¤ Farmer: à¤§à¤¾à¤¨ à¤•à¥€ à¤–à¥‡à¤¤à¥€ à¤•à¥‡ à¤²à¤¿à¤ à¤†à¤¦à¤°à¥à¤¶ à¤¤à¤¾à¤ªà¤®à¤¾à¤¨ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\n",
                  "\n",
                  "ğŸ¤– Advisor: à¤§à¤¾à¤¨ à¤•à¥‡ à¤µà¤¿à¤•à¤¾à¤¸ à¤•à¥‡ à¤²à¤¿à¤ à¤†à¤¦à¤°à¥à¤¶ à¤¤à¤¾à¤ªà¤®à¤¾à¤¨ 20 Â° C à¤¸à¥‡ 35 Â° C à¤•à¥‡ à¤¬à¥€à¤š à¤¹à¥‹à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤‡à¤¸à¥‡ à¤¬à¤¡à¤¼à¥€ à¤®à¤¾à¤¤à¥à¤°à¤¾ à¤®à¥‡à¤‚ à¤ªà¤¾à¤¨à¥€ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ; à¤‡à¤¸à¤²à¤¿à¤, à¤¯à¤¹ à¤­à¤¾à¤°à¥€ à¤µà¤°à¥à¤·à¤¾ (100 à¤¸à¥‡ 200 à¤¸à¥‡à¤®à¥€ à¤¸à¤¾à¤²à¤¾à¤¨à¤¾) à¤µà¤¾à¤²à¥‡ à¤•à¥à¤·à¥‡à¤¤à¥à¤°à¥‹à¤‚ à¤®à¥‡à¤‚ à¤ªà¤¨à¤ªà¤¤à¤¾ à¤¹à¥ˆ à¤¯à¤¾ à¤œà¤¹à¤¾à¤‚ à¤¸à¤¿à¤‚à¤šà¤¾à¤ˆ à¤…à¤šà¥à¤›à¥€ à¤¤à¤°à¤¹ à¤¸à¥‡ à¤µà¤¿à¤•à¤¸à¤¿à¤¤ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆà¥¤ à¤«à¤¸à¤² à¤®à¤¿à¤Ÿà¥à¤Ÿà¥€ à¤¯à¤¾ à¤¦à¥‹à¤®à¤Ÿ à¤®à¤¿à¤Ÿà¥à¤Ÿà¥€ à¤®à¥‡à¤‚ à¤…à¤šà¥à¤›à¥€ à¤¤à¤°à¤¹ à¤¸à¥‡ à¤¬à¤¢à¤¼à¤¤à¥€ à¤¹à¥ˆ à¤œà¥‹ à¤ªà¤¾à¤¨à¥€ à¤•à¥‹ à¤¬à¤¨à¤¾à¤ à¤°à¤– à¤¸à¤•à¤¤à¥€ à¤¹à¥ˆà¥¤ à¤§à¤¾à¤¨ à¤†à¤®à¤¤à¥Œà¤° à¤ªà¤° à¤‰à¤·à¥à¤£à¤•à¤Ÿà¤¿à¤¬à¤‚à¤§à¥€à¤¯ à¤”à¤° à¤‰à¤ªà¥‹à¤·à¥à¤£à¤•à¤Ÿà¤¿à¤¬à¤‚à¤§à¥€à¤¯ à¤•à¥à¤·à¥‡à¤¤à¥à¤°à¥‹à¤‚ à¤®à¥‡à¤‚ à¤‰à¤—à¤¾à¤¯à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤\n",
                  "\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "ğŸ‘¤ Farmer: à¤šà¤¾à¤µà¤² à¤•à¥€ à¤–à¥‡à¤¤à¥€ à¤•à¥‡ à¤²à¤¿à¤ à¤‰à¤ªà¤¯à¥à¤•à¥à¤¤ à¤œà¤²à¤µà¤¾à¤¯à¥ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\n",
                  "\n",
                  "ğŸ¤– Advisor: à¤§à¤¾à¤¨ à¤•à¥‡ à¤µà¤¿à¤•à¤¾à¤¸ à¤•à¥‡ à¤²à¤¿à¤ à¤†à¤¦à¤°à¥à¤¶ à¤¤à¤¾à¤ªà¤®à¤¾à¤¨ 20 Â° C à¤¸à¥‡ 35 Â° C à¤•à¥‡ à¤¬à¥€à¤š à¤¹à¥‹à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤‡à¤¸à¥‡ à¤¬à¤¡à¤¼à¥€ à¤®à¤¾à¤¤à¥à¤°à¤¾ à¤®à¥‡à¤‚ à¤ªà¤¾à¤¨à¥€ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ; à¤‡à¤¸à¤²à¤¿à¤, à¤¯à¤¹ à¤­à¤¾à¤°à¥€ à¤µà¤°à¥à¤·à¤¾ (100 à¤¸à¥‡ 200 à¤¸à¥‡à¤®à¥€ à¤¸à¤¾à¤²à¤¾à¤¨à¤¾) à¤µà¤¾à¤²à¥‡ à¤•à¥à¤·à¥‡à¤¤à¥à¤°à¥‹à¤‚ à¤®à¥‡à¤‚ à¤ªà¤¨à¤ªà¤¤à¤¾ à¤¹à¥ˆ à¤¯à¤¾ à¤œà¤¹à¤¾à¤‚ à¤¸à¤¿à¤‚à¤šà¤¾à¤ˆ à¤…à¤šà¥à¤›à¥€ à¤¤à¤°à¤¹ à¤¸à¥‡ à¤µà¤¿à¤•à¤¸à¤¿à¤¤ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆà¥¤ à¤«à¤¸à¤² à¤®à¤¿à¤Ÿà¥à¤Ÿà¥€ à¤¯à¤¾ à¤¦à¥‹à¤®à¤Ÿ à¤®à¤¿à¤Ÿà¥à¤Ÿà¥€ à¤®à¥‡à¤‚ à¤…à¤šà¥à¤›à¥€ à¤¤à¤°à¤¹ à¤¸à¥‡ à¤¬à¤¢à¤¼à¤¤à¥€ à¤¹à¥ˆ à¤œà¥‹ à¤ªà¤¾à¤¨à¥€ à¤•à¥‹ à¤¬à¤¨à¤¾à¤ à¤°à¤– à¤¸à¤•à¤¤à¥€ à¤¹à¥ˆà¥¤ à¤§à¤¾à¤¨ à¤†à¤®à¤¤à¥Œà¤° à¤ªà¤° à¤‰à¤·à¥à¤£à¤•à¤Ÿà¤¿à¤¬à¤‚à¤§à¥€à¤¯ à¤”à¤° à¤‰à¤ªà¥‹à¤·à¥à¤£à¤•à¤Ÿà¤¿à¤¬à¤‚à¤§à¥€à¤¯ à¤•à¥à¤·à¥‡à¤¤à¥à¤°à¥‹à¤‚ à¤®à¥‡à¤‚ à¤‰à¤—à¤¾à¤¯à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤\n",
                  "\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "ğŸ‘¤ Farmer: à¤§à¤¾à¤¨ à¤•à¥€ à¤–à¥‡à¤¤à¥€ à¤•à¥‡ à¤²à¤¿à¤ à¤‰à¤ªà¤¯à¥à¤•à¥à¤¤ à¤œà¤²à¤µà¤¾à¤¯à¥ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\n",
                  "\n",
                  "ğŸ¤– Advisor: à¤§à¤¾à¤¨ à¤•à¥‡ à¤µà¤¿à¤•à¤¾à¤¸ à¤•à¥‡ à¤²à¤¿à¤ à¤†à¤¦à¤°à¥à¤¶ à¤¤à¤¾à¤ªà¤®à¤¾à¤¨ 20 Â° C à¤¸à¥‡ 35 Â° C à¤•à¥‡ à¤¬à¥€à¤š à¤¹à¥‹à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤‡à¤¸à¥‡ à¤¬à¤¡à¤¼à¥€ à¤®à¤¾à¤¤à¥à¤°à¤¾ à¤®à¥‡à¤‚ à¤ªà¤¾à¤¨à¥€ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ; à¤‡à¤¸à¤²à¤¿à¤, à¤¯à¤¹ à¤­à¤¾à¤°à¥€ à¤µà¤°à¥à¤·à¤¾ (100 à¤¸à¥‡ 200 à¤¸à¥‡à¤®à¥€ à¤¸à¤¾à¤²à¤¾à¤¨à¤¾) à¤µà¤¾à¤²à¥‡ à¤•à¥à¤·à¥‡à¤¤à¥à¤°à¥‹à¤‚ à¤®à¥‡à¤‚ à¤ªà¤¨à¤ªà¤¤à¤¾ à¤¹à¥ˆ à¤¯à¤¾ à¤œà¤¹à¤¾à¤‚ à¤¸à¤¿à¤‚à¤šà¤¾à¤ˆ à¤…à¤šà¥à¤›à¥€ à¤¤à¤°à¤¹ à¤¸à¥‡ à¤µà¤¿à¤•à¤¸à¤¿à¤¤ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆà¥¤ à¤«à¤¸à¤² à¤®à¤¿à¤Ÿà¥à¤Ÿà¥€ à¤¯à¤¾ à¤¦à¥‹à¤®à¤Ÿ à¤®à¤¿à¤Ÿà¥à¤Ÿà¥€ à¤®à¥‡à¤‚ à¤…à¤šà¥à¤›à¥€ à¤¤à¤°à¤¹ à¤¸à¥‡ à¤¬à¤¢à¤¼à¤¤à¥€ à¤¹à¥ˆ à¤œà¥‹ à¤ªà¤¾à¤¨à¥€ à¤•à¥‹ à¤¬à¤¨à¤¾à¤ à¤°à¤– à¤¸à¤•à¤¤à¥€ à¤¹à¥ˆà¥¤ à¤§à¤¾à¤¨ à¤†à¤®à¤¤à¥Œà¤° à¤ªà¤° à¤‰à¤·à¥à¤£à¤•à¤Ÿà¤¿à¤¬à¤‚à¤§à¥€à¤¯ à¤”à¤° à¤‰à¤ªà¥‹à¤·à¥à¤£à¤•à¤Ÿà¤¿à¤¬à¤‚à¤§à¥€à¤¯ à¤•à¥à¤·à¥‡à¤¤à¥à¤°à¥‹à¤‚ à¤®à¥‡à¤‚ à¤‰à¤—à¤¾à¤¯à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤\n",
                  "\n"
                ]
              }
            ]
          }
        },
        "8e0883cf670741ceb7f84a4c38669006": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "917d41b7d6f04e5da4fe66ff17606655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "80%"
          }
        },
        "ff40d37c81754933ab3a4655850b91b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7f66973ea904135a0c9279595f7bd3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install langchain\n",
        "#!pip install langchain langchain-community\n",
        "#!pip install langchain-ollama\n",
        "#!pip install sentence-transformers faiss-cpu tiktoken python-dotenv -- HuggingFaceâ€™s library for loading and running LLMs.\n",
        "#! pip install langchain_huggingface\n",
        "##! pip install faiss-cpu\n",
        "# Install required packages\n",
        "!pip install langdetect\n",
        "! pip install googletrans\n",
        "#!pip install indicnlp\n",
        "! pip install -U langchain-community\n",
        "! pip install pypdf\n",
        "! pip install -U langchain-huggingface\n",
        "!pip install stanza\n",
        "!pip install polyglot\n",
        "!pip install pyicu pycld2 morfessor\n",
        "!pip install deep_translator"
      ],
      "metadata": {
        "id": "YWq0J5h_ca1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bfa8598-c47e-4ca9-9b03-28de33b6422f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m286.7/981.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=023f45d49c41302eba19aded9cc665399eec9d6c79be2dc5bcb3cd1e99c51655\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Collecting googletrans\n",
            "  Downloading googletrans-4.0.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: httpx>=0.27.2 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.27.2->googletrans) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.27.2->googletrans) (4.3.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.15.0)\n",
            "Downloading googletrans-4.0.2-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: googletrans\n",
            "Successfully installed googletrans-4.0.2\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.0.1 (from langchain-community)\n",
            "  Downloading langchain_core-1.0.5-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.42)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
            "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.0.5-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-core, langchain-text-splitters, langchain-classic, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.79\n",
            "    Uninstalling langchain-core-0.3.79:\n",
            "      Successfully uninstalled langchain-core-0.3.79\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.11\n",
            "    Uninstalling langchain-text-splitters-0.3.11:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.5 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.0.5 langchain-text-splitters-1.0.0 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.2.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading pypdf-6.2.0-py3-none-any.whl (326 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m326.6/326.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-6.2.0\n",
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-1.0.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.36.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (1.0.5)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.22.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (0.4.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (2.11.10)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (8.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.10.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (1.3.1)\n",
            "Downloading langchain_huggingface-1.0.1-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: langchain-huggingface\n",
            "Successfully installed langchain-huggingface-1.0.1\n",
            "Collecting stanza\n",
            "  Downloading stanza-1.11.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting emoji (from stanza)\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from stanza) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (5.29.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from stanza) (2.32.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from stanza) (3.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from stanza) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->stanza) (3.0.3)\n",
            "Downloading stanza-1.11.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji, stanza\n",
            "Successfully installed emoji-2.15.0 stanza-1.11.0\n",
            "Collecting polyglot\n",
            "  Downloading polyglot-16.7.4.tar.gz (126 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: polyglot\n",
            "  Building wheel for polyglot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for polyglot: filename=polyglot-16.7.4-py2.py3-none-any.whl size=52563 sha256=37948e91e229bd459055691419f30309f4dd740f319a02382c09db72f1799072\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/5e/28/47349211ec1f91379f41ed10bc2520f7071ecfb6cbe182f6fe\n",
            "Successfully built polyglot\n",
            "Installing collected packages: polyglot\n",
            "Successfully installed polyglot-16.7.4\n",
            "Collecting pyicu\n",
            "  Downloading pyicu-2.16.tar.gz (268 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m268.1/268.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycld2\n",
            "  Downloading pycld2-0.42-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
            "Downloading pycld2-0.42-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Building wheels for collected packages: pyicu\n",
            "  Building wheel for pyicu (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyicu: filename=pyicu-2.16-cp312-cp312-linux_x86_64.whl size=2721121 sha256=21d774d0580e97b27a555b117e17d008adae1c125e82d5ae68c3f3890c49f311\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/d3/8e/efa19e1de2f9cf1451baf994a55dddcaa41a94185f2501ca08\n",
            "Successfully built pyicu\n",
            "Installing collected packages: pyicu, pycld2, morfessor\n",
            "Successfully installed morfessor-2.0.6 pycld2-0.42 pyicu-2.16\n",
            "Collecting deep_translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.12/dist-packages (from deep_translator) (4.13.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from deep_translator) (2.32.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.10.5)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep_translator\n",
            "Successfully installed deep_translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''import asyncio, threading\n",
        "\n",
        "class AsyncRunner:\n",
        "    def __init__(self):\n",
        "        self.loop = asyncio.new_event_loop()\n",
        "        t = threading.Thread(target=self.loop.run_forever, daemon=True)\n",
        "        t.start()\n",
        "\n",
        "    def run(self, coro):\n",
        "        return asyncio.run_coroutine_threadsafe(coro, self.loop)\n",
        "\n",
        "# Create one global runner\n",
        "runner = AsyncRunner()'''\n"
      ],
      "metadata": {
        "id": "tR8f_oTH32Td",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "a8971b97-f223-424d-d112-2af562dc5067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import asyncio, threading\\n\\nclass AsyncRunner:\\n    def __init__(self):\\n        self.loop = asyncio.new_event_loop()\\n        t = threading.Thread(target=self.loop.run_forever, daemon=True)\\n        t.start()\\n\\n    def run(self, coro):\\n        return asyncio.run_coroutine_threadsafe(coro, self.loop)\\n\\n# Create one global runner\\nrunner = AsyncRunner()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Auto-install FAISS if not already installed\n",
        "try:\n",
        "    import faiss\n",
        "except ImportError:\n",
        "    import subprocess, sys\n",
        "    # Check if GPU is available\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.cuda.is_available():\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"faiss-gpu\", \"--quiet\"])\n",
        "            print(\"âœ… Installed faiss-gpu\")\n",
        "        else:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"faiss-cpu\", \"--quiet\"])\n",
        "            print(\"âœ… Installed faiss-cpu\")\n",
        "    except:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"faiss-cpu\", \"--quiet\"])\n",
        "        print(\"âœ… Installed faiss-cpu\")\n",
        "\n",
        "# Test import\n",
        "import faiss\n",
        "print(f\"FAISS version: {faiss.__version__}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoiS4AnismuA",
        "outputId": "a38b775c-ef4e-47c9-88d3-e5b7f8684903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Installed faiss-cpu\n",
            "FAISS version: 1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pkg_resources\n",
        "print(pkg_resources.get_distribution(\"langchain\").version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqtAG_loQF2M",
        "outputId": "69ac6e79-7902-4a77-b34f-83019d94e042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-441797147.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# ğŸ”¹ NLP + Utilities\n",
        "# -------------------------------\n",
        "import re\n",
        "import os\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import pandas as pd\n",
        "from langdetect import detect\n",
        "from deep_translator import GoogleTranslator\n",
        "from google.colab import files\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# -------------------------------\n",
        "# ğŸ”¹ Stanza (Indian language sentence splitting)\n",
        "# -------------------------------\n",
        "import stanza\n",
        "\n",
        "# -------------------------------\n",
        "# ğŸ”¹ Transformers (HuggingFace Models)\n",
        "# -------------------------------\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    pipeline,\n",
        "    MarianMTModel,\n",
        "    MarianTokenizer\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# ğŸ”¹ LangChain (0.3.27 Compatible Imports)\n",
        "# -------------------------------\n",
        "\n",
        "# Document Loaders (community)\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader,\n",
        "    PyPDFLoader,\n",
        "    CSVLoader\n",
        ")\n",
        "\n",
        "# Embeddings (community)\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Vectorstore (community)\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Prompt templates\n",
        "from langchain_core.prompts import PromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Runnables (LCEL)\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "# Chat history (community)\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "# Message types\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Chains (new LC architecture)\n",
        "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n"
      ],
      "metadata": {
        "id": "J9ZGjr3UREEB",
        "outputId": "13a97051-0c61-4aad-8483-2736ffe8b001",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_core.memory'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3379353910.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Chains (new LC architecture)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory_aware_retriever\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_history_aware_retriever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_documents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_stuff_documents_chain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_retrieval_chain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/chains/combine_documents/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"Different ways to combine documents.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m from langchain.chains.combine_documents.reduce import (\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0macollapse_docs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcollapse_docs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/chains/combine_documents/reduce.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_documents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseCombineDocumentsChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/chains/combine_documents/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mDEFAULT_DOCUMENT_SEPARATOR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mCallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRunInfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m from langchain_core.runnables import (\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core.memory'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”¹ Step 2: Imports\n",
        "import stanza\n",
        "import re\n",
        "#import stanza\n",
        "#stanza.download(\"bn\", model_dir=\"./stanza_resources\")\n",
        "#from langchain.embeddings import HuggingFaceEmbeddings\n",
        "#from langchain.vectorstores import FAISS\n",
        "#from langchain.llms import HuggingFacePipeline\n",
        "#from langchain.chains import RetrievalQA\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline , MarianMTModel, MarianTokenizer\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader, CSVLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter #Splits documents into smaller chunks for embedding.\n",
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "#from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_community.vectorstores import FAISS #Vector database for storing and retrieving document embeddings.\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader, CSVLoader\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import os\n",
        "import asyncio #Allows async/await so translations and queries can run efficiently.\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "#from langchain.memory import ConversationBufferMemory # Corrected import - REMOVED\n",
        "import requests\n",
        "#from langdetect import detect\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "#3from langchain.prompts import PromptTemplate\n",
        "#from langchain.chains import LLMChain\n",
        "#from langchain.memory import ConversationBufferMemory\n",
        "#from langchain.chains import ConversationalRetrievalChain # REMOVED\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "import nest_asyncio, asyncio\n",
        "nest_asyncio.apply()\n",
        "#import nest_asyncio, asyncio\n",
        "#import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from deep_translator import GoogleTranslator\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ğŸ”¹ Step 3: Upload and load document (PDF, CSV, or TXT)\n",
        "uploaded = files.upload()\n",
        "file_path = list(uploaded.keys())[0]\n",
        "file_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "zI9-WEpK2EEh",
        "outputId": "3d8cb316-8d90-4649-c782-abcb9f013997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_core.memory'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2791509048.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_history\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInMemoryChatMessageHistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_history_aware_retriever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_documents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_stuff_documents_chain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_retrieval_chain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest_asyncio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/chains/combine_documents/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"Different ways to combine documents.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m from langchain.chains.combine_documents.reduce import (\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0macollapse_docs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcollapse_docs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/chains/combine_documents/reduce.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_documents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseCombineDocumentsChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/chains/combine_documents/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mDEFAULT_DOCUMENT_SEPARATOR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mCallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRunInfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m from langchain_core.runnables import (\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core.memory'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”¹ Step 3: Upload and load document (PDF, CSV, or TXT)\n",
        "if file_path.endswith(\".pdf\"):\n",
        "    loader = PyPDFLoader(file_path)\n",
        "elif file_path.endswith(\".csv\"):\n",
        "    loader = CSVLoader(file_path)\n",
        "else:\n",
        "    loader = TextLoader(file_path, encoding='utf-8')\n",
        "\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
        "split_docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "3x17stwyo9VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LLM (Flan-T5 large)\n",
        "\"\"\"Loads Flan-T5 Large model from HuggingFace.\n",
        "   Wraps it in a LangChain-compatible HuggingFacePipeline.\"\"\"\n",
        "\n",
        "llm_model_id = \"google/flan-t5-large\"\n",
        "#MODEL_NAME = \"your-hf-model-name\"  # e.g. \"bert-base-uncased\" or the HF model you're using\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_model_id,use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_id)\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256)\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Load FAISS retriever\n",
        "\"\"\"Uses BAAI/bge-m3 embeddings (multilingual-capable).\n",
        "    Creates a FAISS vector store from split_docs (your preprocessed dataset).\n",
        "    Turns FAISS into a retriever for document search.\"\"\"\n",
        "\n",
        "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\") #multilingual embeddings\n",
        "#vectorstore = FAISS.load_local(\"faiss_index\", embed_model, allow_dangerous_deserialization=True)\n",
        "vectorstore = FAISS.from_documents(split_docs, embedding=embed_model)\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stNo5oDt3uqK",
        "outputId": "523e58a7-e4e4-4f0a-d715-b8d9e8e11cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"In the context of the error you saw\n",
        "(AttributeError: 'coroutine' object has no attribute 'text'), the translator.translate method,\n",
        "when called directly, was returning a coroutine object. A coroutine is a specific type of\n",
        "awaitable object. You can't directly access attributes like .text on the coroutine itself\n",
        "because it represents a future result, not the result right away. You need to await it first\n",
        "to get the actual result which then would have the .text attribute\"\"\"\n",
        "\n",
        "# Cell 6: Setup LangChain Conversational Chain with Memory\n",
        "#memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 3. CHAT UI WIDGETS\n",
        "# ============================\n",
        "chat_box = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Ask about crops, climate, fertilizers...',\n",
        "    description='ğŸŒ¾ Farmer:',\n",
        "    layout={'width': '80%'}\n",
        ")\n",
        "output_box = widgets.Output()\n",
        "\n",
        "# Translator setup -  Google translate client instance created\n",
        "translator = Translator()\n",
        "\n",
        "# -------------------------\n",
        "#  GLOBAL SESSION STORE\n",
        "# -------------------------\n",
        "_session_store = {}\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "  if session_id not in _session_store:\n",
        "    _session_store[session_id] = InMemoryChatMessageHistory()\n",
        "  return _session_store[session_id]\n",
        "\n",
        "def n_tokens(text: str) -> int:\n",
        "  return len(tokenizer.encode(text, add_special_tokens=False))\n",
        "\n",
        "def trim_text_by_tokens(text: str, allowed_tokens: int) -> str:\n",
        "  tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "  if len(tokens) <= allowed_tokens:\n",
        "      return text\n",
        "  # keep the last allowed_tokens tokens (you can keep first tokens instead)\n",
        "  kept = tokens[-allowed_tokens:]\n",
        "  return tokenizer.decode(kept, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "# Helper to translate synchronously\n",
        "'''async def safe_translate(text, src_lang, dest_lang): # Made async\n",
        "    result = translator.translate(text, src=src_lang, dest=dest_lang)\n",
        "    return result.text  # always return plain string'''\n",
        "'''def safe_translate(text, src_lang, dest_lang):\n",
        "    try:\n",
        "        result = translator.translate(text, src=src_lang, dest=dest_lang)\n",
        "        return result.text\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error: {e}\")\n",
        "        return text'''\n",
        "\n",
        "def translate_to_english(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Translate any language query into English using deep-translator.\n",
        "    \"\"\"\n",
        "    return GoogleTranslator(source=\"auto\", target=\"en\").translate(query)\n",
        "\n",
        "def translate_back_to_user_lang(answer: str, target_lang: str) -> str:\n",
        "    \"\"\"\n",
        "    Translate the English answer back to the user's original language.\n",
        "\n",
        "    Args:\n",
        "        answer (str): The English output from LLM.\n",
        "        target_lang (str): Target language code (e.g., 'hi', 'bn', 'ta', 'te').\n",
        "\n",
        "    Returns:\n",
        "        str: Translated text in the target language.\n",
        "    \"\"\"\n",
        "    return GoogleTranslator(source=\"en\", target=target_lang).translate(answer)\n",
        "\n",
        "def lang_translator(text,SUPPORTED_STANZA,lang=\"en\",output_style=\"bullets\"):\n",
        "      sentences = []\n",
        "      # --- Try Stanza first ---\n",
        "      if lang in SUPPORTED_STANZA:\n",
        "          try:\n",
        "              import stanza\n",
        "              nlp = stanza.Pipeline(lang, model_dir=\"./stanza_resources\", processors=\"tokenize\", verbose=False)\n",
        "              doc = nlp(text)\n",
        "              sentences = [s.text for s in doc.sentences]\n",
        "          except Exception as e:\n",
        "              print(f\"[Stanza failed â†’ {e}. Trying Polyglot]\")\n",
        "\n",
        "      # --- If no sentences yet, try Polyglot ---\n",
        "      elif not sentences:\n",
        "          try:\n",
        "              from polyglot.text import Text\n",
        "              poly_text = Text(text, hint_language_code=lang)\n",
        "              sentences = [str(s) for s in poly_text.sentences]\n",
        "          except Exception as e:\n",
        "              print(f\"[Polyglot failed â†’ {e}. Falling back to regex]\")\n",
        "\n",
        "      # --- Final fallback: regex split ---\n",
        "      elif not sentences:\n",
        "          sentences = re.split(r'[à¥¤.?!]', text)\n",
        "\n",
        "      # --- Clean up ---\n",
        "      #print(f\"sentences 1:{sentences}\")\n",
        "\n",
        "      sentences = [s.strip() for s in sentences if len(s.strip()) > 3]\n",
        "\n",
        "      #print(f\"sentences 2:{sentences}\")\n",
        "\n",
        "      # --- Format output ---\n",
        "      if output_style == \"bullets\":\n",
        "          return \"\\n\".join([f\"- {s}\" for s in sentences[:5]])\n",
        "      elif output_style == \"numbered\":\n",
        "          return \"\\n\".join([f\"{i+1}. {s}\" for i, s in enumerate(sentences[:5])])\n",
        "      else:\n",
        "          return text\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 5. FOLLOW-UP QUESTION GENERATOR\n",
        "# ============================\n",
        "def suggest_questions(answer_en: str, user_lang: str, output_style: str, supported_lang: list): # Made async\n",
        "    \"\"\"Generate 3 follow-up questions from the Advisor's English response,\n",
        "       then translate them back to the farmer's language.\"\"\"\n",
        "        #print(\"test 1...\")\n",
        "    try:\n",
        "        #print(\"test 2...\")\n",
        "        suggestion_prompt = (\n",
        "            f\"Based on this farming answer:\\n\\n{answer_en}\\n\\n\"\n",
        "            \"Suggest 3 short, specific follow-up farming questions the farmer might ask next. \"\n",
        "            \"They should be about crops, soil, irrigation, fertilizers, or climate.\"\n",
        "        )\n",
        "        #print(\"test 3...\")\n",
        "        suggestions_en = llm.invoke(suggestion_prompt)\n",
        "        suggestions_list = [s.strip(\"-â€¢0123456789. \") for s in suggestions_en.split(\"\\n\") if s.strip()]\n",
        "        suggestions_list = suggestions_list[:3]  # keep top 3\n",
        "        #print(\"test 4...\")\n",
        "        # Translate each suggestion\n",
        "        suggestions_translated = [\n",
        "            #lang_translator(s,supported_lang,lang=user_lang,output_style=output_style) if user_lang != \"en\" else s\n",
        "            #for s in suggestions_list\n",
        "            translate_back_to_user_lang(s,user_lang) if user_lang != \"en\" else s for s in suggestions_list\n",
        "        ]\n",
        "        #print(\"test 5...\")\n",
        "        return suggestions_translated\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "qa_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"input\",\"context\",\"tone\",\"user_lang\",\"style_instructions\",\"chat_history\"],\n",
        "    #input_variables=[\"input\",\"context\",\"chat_history\"],\n",
        "    template=\"\"\"\n",
        "    You are an agricultural expert helping Indian farmers.\n",
        "Answer the question based on the context retrieved.\n",
        "The answer should be to the point and precise.\n",
        "Avoid repeating facts in the response.\n",
        "Use a {tone} tone.\n",
        "Adhere to this answering format : {style_instructions}\n",
        "Respond in {user_lang} language(same as user query).\n",
        "Avoid hallucinating facts not present in the context.\n",
        "\n",
        "Question:\n",
        "{input}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "\n",
        "If the answer is not in the context, reply: \"I could not find this information.\"\n",
        "\"\"\"\n",
        ")\n",
        "#\"chat_history\"\n",
        "#Chat History:\n",
        "#{chat_history}\n",
        "\n",
        "def process_query(user_query: str,tone: str,output_style: str,supported_lang): # Made async\n",
        "    \"\"\"Detects language, translates to English for QA, then back to user language.\n",
        "       Also forces detailed answers if response is too short.\"\"\"\n",
        "    # Translate query to English\n",
        "    #query_en = translator.translate(query_text, target=\"en\", source=user_lang) if user_lang != \"en\" else query_text\n",
        "     # Detect language\n",
        "    user_lang = detect(user_query)\n",
        "    print(f\"Detected language: {user_lang}\")\n",
        "    #print(\"Inside process query before transalting input query...\")\n",
        "    #query_en = safe_translate(user_query, user_lang, \"en\") if user_lang != \"en\" else user_query # Await safe_translate\n",
        "    #query_en = await translator.translate(user_query, src=user_lang, dest=\"en\") if user_lang != \"en\" else user_query\n",
        "    query_en = translate_to_english(user_query) if user_lang != \"en\" else user_query\n",
        "    print(f\"Translated query:{query_en}\")\n",
        "\n",
        "\n",
        "    # Retrieve docs in English\n",
        "    \"\"\"Uses FAISS retriever to get relevant documents based on the English query.\n",
        "    Joins the page_content of all retrieved docs into a single context string.\"\"\"\n",
        "    docs = retriever.invoke(query_en,top_k=2)\n",
        "    #print(\"Inside pq 1\")\n",
        "    #context = \" \".join([doc.page_content for doc in docs])\n",
        "    if not docs or all(len(doc.page_content.strip()) == 0 for doc in docs):\n",
        "        not_found_message = \"I could not find enough information in the provided data.\"\n",
        "    #print(\"Inside pq 2\")\n",
        "    # Use the model's tokenizer (transformers) to count tokens and trim exactly to the model limit.\n",
        "    #This ensures the total input remains <= model limit.\n",
        "    # inside process_query:\n",
        "\n",
        "    style_instructions = {\n",
        "    \"bullets\": \"\"\"- Format your answer STRICTLY as 1-2 bullet points.\n",
        "- Each bullet point must be one sentence only.\n",
        "- Do not write paragraphs.\n",
        "- Do not repeat the context.\n",
        "- Only include the most relevant facts for the question.\"\"\",\n",
        "\n",
        "    \"numbered\": \"\"\"- Format your answer STRICTLY as a numbered list (1-2 items).\n",
        "- Each item must be one sentence only.\n",
        "- Do not write paragraphs or extra text.\"\"\",\n",
        "\n",
        "    \"table\": \"\"\"- Format your answer STRICTLY as a markdown table.\n",
        "- Use only 2â€“4 rows with concise entries.\n",
        "- Do not add unrelated details.\"\"\",\n",
        "}.get(output_style, \"\"\"- Provide the answer in 1-2 concise sentences only.\n",
        "- No repetition of the context.\"\"\")\n",
        "\n",
        "    #from transformers import AutoTokenizer\n",
        "    vars_dict = {\n",
        "      \"input\": user_query,\n",
        "      \"context\": \"\",\n",
        "      \"tone\": tone,\n",
        "      \"user_lang\": user_lang,\n",
        "      \"style_instructions\": style_instructions,\n",
        "      \"chat_history\": \"\"\n",
        "      }\n",
        "    prompt_prefix = qa_prompt_template.format(**vars_dict)\n",
        "    '''prompt_prefix = qa_prompt_template.format(\n",
        "        input=user_query,\n",
        "        context=\"\",\n",
        "        tone=tone,\n",
        "        user_lang=user_lang,\n",
        "        style_instructions=style_instructions\n",
        "        )'''\n",
        "\n",
        "    prompt_tokens_est = n_tokens(prompt_prefix)\n",
        "    MODEL_MAX_TOKENS = 512    # from the error\n",
        "    ANSWER_TOKEN_BUDGET = 128\n",
        "    allowed_context_tokens = MODEL_MAX_TOKENS - prompt_tokens_est - ANSWER_TOKEN_BUDGET\n",
        "    if allowed_context_tokens < 50:\n",
        "      allowed_context_tokens = 50\n",
        "\n",
        "    context = \" \".join([doc.page_content for doc in docs])\n",
        "    trimmed_context = trim_text_by_tokens(context, allowed_context_tokens)\n",
        "    '''max_context_chars = 1500\n",
        "    trimmed_context = context[:max_context_chars]'''\n",
        "    #vars_dict[\"context\"] = trimmed_context\n",
        "    #final_prompt = qa_prompt_template.format(**vars_dict)\n",
        "\n",
        "    #####print(f\"Trimmed context:{trimmed_context}\")\n",
        "\n",
        "\n",
        "    #print(\"Inside pq 3\")\n",
        "\n",
        "    print(\">>> prompt tokens:\", prompt_tokens_est)\n",
        "    print(\">>> context chars:\", len(context))\n",
        "    print(\">>> allowed_context_tokens:\", allowed_context_tokens)\n",
        "\n",
        "    '''# Create LLMChain with your prompt\n",
        "    qa_llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "    # âœ… Instead of LLMChain, use prompt | llm -> Instead of using LLMChain, you should compose your prompt and LLM into a runnable using the new style:\n",
        "    #This happens because in LangChain >0.1.17, ConversationalRetrievalChain internally expects the LLM chain to be Runnable (not LLMChain).\n",
        "\n",
        "    # Build ConversationalRetrievalChain manually\n",
        "    qa_chain = ConversationalRetrievalChain(\n",
        "    retriever=retriever,\n",
        "    combine_docs_chain=qa_llm_chain,\n",
        "    memory=memory,\n",
        "    return_source_documents=True  # optional\n",
        "    )'''\n",
        "    #print(f\"Inside pq 4 - {final_prompt}\")\n",
        "    #print(f\"Inside pq 4 - {type(final_prompt)}\")\n",
        "    # Create retriever - History-aware retriever\n",
        "    history_aware_retriever = create_history_aware_retriever(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        prompt=qa_prompt_template\n",
        "        #prompt = final_prompt\n",
        "    )\n",
        "    #print(\"Inside pq 5 -\")\n",
        "    # Documents chain - Stuff documents into answer chain\n",
        "    doc_chain = create_stuff_documents_chain(llm,qa_prompt_template)\n",
        "    #doc_chain = create_stuff_documents_chain(llm,final_prompt)\n",
        "    #print(\"Inside pq 6 -\")\n",
        "    #Final QA chain\n",
        "    qa_chain = create_retrieval_chain(\n",
        "        history_aware_retriever,\n",
        "        doc_chain\n",
        "    )\n",
        "    '''Please note qa_chain.invoke(chat_history:..) is for manually fetching and\n",
        "    feeding the response and queries to memory, however  qa_chain_with_memory automatically takes care of it'''\n",
        "    # âœ… Wrap with memory - REMOVED: Manual memory management\n",
        "    # qa_chain_with_memory = RunnableWithMessageHistory(\n",
        "    #     qa_chain,\n",
        "    #     lambda session_id: memory,   # use one memory object per session\n",
        "    #     input_messages_key=\"input\",\n",
        "    #     history_messages_key=\"chat_history\",\n",
        "    #     output_messages_key=\"answer\"\n",
        "    # )\n",
        "\n",
        "    # Call retrieval QA chain - Using the base qa_chain and passing history manually\n",
        "    # response_en = qa_chain_with_memory.invoke( # REMOVED: Manual memory management\n",
        "    # Store per-session chat histories\n",
        "    session_id = \"farmer1\"\n",
        "\n",
        "\n",
        "    qa_chain_with_history = RunnableWithMessageHistory(\n",
        "     qa_chain,\n",
        "    get_session_history ,   # <-- IMPORTANT: must return ChatMessageHistory, not ConversationBufferMemory\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        "    output_messages_key=\"answer\"\n",
        "    )\n",
        "    '''call the chain asynchronously inside your async function'''\n",
        "\n",
        "\n",
        "    '''def add_defaults(inputs):\n",
        "      inputs[\"tone\"] = inputs.get(\"tone\", \"formal\")\n",
        "      inputs[\"user_lang\"] = inputs.get(\"user_lang\", \"en\")\n",
        "      inputs[\"style_instructions\"] = inputs.get(\"style_instructions\", \"concise\")\n",
        "      return inputs\n",
        "\n",
        "    qa_chain_with_defaults = RunnableLambda(add_defaults) | qa_chain_with_history\n",
        "\n",
        "    response_en = qa_chain_with_defaults.invoke(\n",
        "      {\"input\": query_en, \"context\": trimmed_context},\n",
        "      config={\"configurable\": {\"session_id\": session_id}}\n",
        "      )'''\n",
        "\n",
        "    response_en = qa_chain_with_history.invoke( # MODIFIED: Using base chain\n",
        "      {\n",
        "      #\"chat_history\": formatted_history, # ADDED: Explicitly pass history\n",
        "      \"context\": trimmed_context,\n",
        "      \"input\": query_en,\n",
        "      \"tone\": tone,\n",
        "      \"user_lang\": user_lang,\n",
        "      \"style_instructions\": style_instructions\n",
        "      },\n",
        "      config={\"configurable\": {\"session_id\": session_id}}\n",
        "    )\n",
        "\n",
        "\n",
        "    '''Your retriever uses history (via create_history_aware_retriever)\n",
        "    Your memory keeps track of chat (RunnableWithMessageHistory)\n",
        "    The UI loop stays the same, just call qa_chain_with_memory.invoke(...) instead of qa_chain.invoke(...)'''\n",
        "    #print('The response is : ')\n",
        "    #print(response_en[\"answer\"])\n",
        "\n",
        "    #response_en = qa_chain.invoke({\"input\": query_en, \"chat_history\": memory.chat_memory.messages})\n",
        "    #print(response[\"answer\"])   # Final answer\n",
        "\n",
        "    # âœ… Ensure response is meaningful (avoid \"Yes\"/\"Helpful\")\n",
        "    if not response_en or len(response_en[\"answer\"].strip()) < 15:\n",
        "        response_en[\"answer\"] = llm(\n",
        "            f\"Farmer asked: {query_en}\\n\\n\"\n",
        "            \"The previous answer was too short. Give a clear and helpful agricultural explanation in 2â€“3 sentences.\"\n",
        "        )\n",
        "\n",
        "    '''# Normalize LLM output\n",
        "    if isinstance(response_en, str):\n",
        "        eng_answer_str = response_en\n",
        "    elif hasattr(response_en, \"content\"):\n",
        "        eng_answer_str = response_en.content\n",
        "    elif isinstance(response_en, list) and len(response_en) > 0:\n",
        "        eng_answer_str = response_en[0].text\n",
        "    else:\n",
        "        eng_answer_str = str(response_en)'''\n",
        "    eng_answer_str = response_en[\"answer\"]\n",
        "\n",
        "    '''# Translate answer back to original language\n",
        "    if user_lang != \"en\":\n",
        "        final_answer = await safe_translate(eng_answer_str, \"en\", user_lang)\n",
        "    else:\n",
        "        final_answer = eng_answer_str'''\n",
        "\n",
        "    #SUPPORTED_STANZA = [\"hi\", \"ta\", \"te\", \"ur\"]\n",
        "    #query_lang_answer = lang_translator(eng_answer_str,supported_lang,lang=user_lang,output_style=output_style)\n",
        "    query_lang_answer = translate_back_to_user_lang(eng_answer_str,user_lang) if user_lang != \"en\" else eng_answer_str\n",
        "    #print(f\" response :{eng_answer_str}\")\n",
        "\n",
        "    '''# ğŸ”¹ Save latest Q&A into memory - ADDED: Manual memory update\n",
        "    memory.save_context(\n",
        "        {\"input\": user_query},    # in original user language\n",
        "        {\"output\": query_lang_answer}  # answer in same language\n",
        "    )'''\n",
        "\n",
        "    return user_lang, user_query, query_lang_answer, eng_answer_str  # English kept for suggestion generator\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 6. CHAT HANDLER\n",
        "# ============================\n",
        "#import asyncio # Import asyncio\n",
        "def on_submit_handler(user_input,tone,op_style,supported_lang): # Made async\n",
        "    #SUPPORTED_STANZA = [\"hi\", \"ta\", \"te\", \"ur\"]\n",
        "    #supported_lang = SUPPORTED_STANZA\n",
        "    #try:\n",
        "    '''user_input = query.value.strip()\n",
        "    query.value = \"\"'''\n",
        "    #print(\"osa test 1...\")\n",
        "    #print(f\"âœ… Entered on_submit_async with query: {user_input}\")  # user_input is already a string\n",
        "    if not user_input:\n",
        "        return\n",
        "    #print('osa test 2...')\n",
        "    with output_box:\n",
        "      #print('osa test 3...')\n",
        "      clear_output(wait=True)\n",
        "      #print(\"ğŸš€ on_submit_handler started\")   # confirm entry\n",
        "      print(f\"User asked: {user_input}\")\n",
        "      #try:\n",
        "      if not user_input:\n",
        "        return\n",
        "\n",
        "      # âœ… Reset command\n",
        "      if user_input.lower() in [\"reset\", \"/reset\"]:\n",
        "          _session_store[\"farmer1\"] = InMemoryChatMessageHistory()\n",
        "          #get_session_history() = InMemoryChatMessageHistory()\n",
        "          #memory.clear()\n",
        "          print(\"âœ… Conversation history cleared\")\n",
        "          return\n",
        "\n",
        "      #try:\n",
        "      # Run multilingual process\n",
        "      lang, original_q, answer, answer_en = process_query(user_input,tone,op_style,supported_lang) # Await process_query\n",
        "\n",
        "      # âœ… LangChain already stores history (no need to manually add Q/A)\n",
        "\n",
        "      # Show conversation\n",
        "      print(\"=== Conversation ===\\n\")\n",
        "      prev_msg = None\n",
        "\n",
        "      #chat_history = memory.load_memory_variables({}).get(\"chat_history\", [])\n",
        "      #chat_history = _session_store[\"farmer1\"].messages if \"farmer1\" in _session_store else []\n",
        "\n",
        "      #chat_history = _session_store[\"farmer1\"].messages\n",
        "      chat_history = get_session_history(\"farmer1\").messages\n",
        "\n",
        "      for msg in chat_history:\n",
        "        #print(\"-- This is the msg content -- \")\n",
        "        #print(msg)\n",
        "        if msg.content != prev_msg:  # avoid duplicate display\n",
        "          role = \"ğŸ‘¤ Farmer\" if msg.type == \"human\" else \"ğŸ¤– Advisor\"\n",
        "          # Translate only if needed\n",
        "          display_text = (\n",
        "              translate_back_to_user_lang(msg.content,lang) if lang != \"en\" else msg.content\n",
        "          )\n",
        "          #print(f\"msg content - {msg.content}\")\n",
        "          #print(f\"lang - {lang}\")\n",
        "          #print(f\"{role}: {msg.content}\\n\")\n",
        "          print(f\"{role}: {display_text}\\n\")\n",
        "          prev_msg = msg.content\n",
        "\n",
        "      # âœ… Show follow-up suggestions\n",
        "      followups = suggest_questions(answer, lang,op_style,supported_lang) # Await suggest_questions\n",
        "      if followups:\n",
        "          print(\"ğŸ’¡ Suggested Questions:\")\n",
        "          for f in followups:\n",
        "            print(f\"ğŸ‘‰ {f}\")\n",
        "\n",
        "      '''except Exception as e:\n",
        "        #with output_box:\n",
        "        print(f\"âŒ Error inside on_submit_async: {e}\")\n",
        "        #print(f\"âŒ Error: {str(e)}\")\n",
        "        #print(\"ğŸ¤– Please try again\")'''\n",
        "\n",
        "# ============================\n",
        "# 7. CONNECT HANDLER TO CHATBOX\n",
        "# ============================\n",
        "SUPPORTED_LANGS = [\"hi\", \"ta\", \"te\", \"ur\"]\n",
        "# Update on_submit handler to run the async function\n",
        "#chat_box.on_submit(lambda query: asyncio.create_task(on_submit_async(query, \"formal\", 'bullets', SUPPORTED_LANGS))) # REMOVED asyncio.run\n",
        "#chat_box.on_submit(lambda query: asyncio.run(on_submit(query, \"formal\", 'bullets', SUPPORTED_LANGS))) # ADDED asyncio.ensure_future\n",
        "'''def on_submit(query):\n",
        "    asyncio.ensure_future(on_submit_async(query, \"formal\", \"bullets\", SUPPORTED_LANGS))'''\n",
        "# Sync wrapper for chatbox enter key\n",
        "def on_submit(widget):\n",
        "    query_text = widget.value.strip()\n",
        "    widget.value = \"\"   # clear after hitting Enter\n",
        "    #print(\"âš¡ on_submit called with:\", query_text)\n",
        "    if query_text:\n",
        "      #loop = asyncio.get_event_loop()\n",
        "      #loop.create_task(on_submit_async(query_text, \"formal\", \"bullets\", SUPPORTED_LANGS))\n",
        "      #runner.run(on_submit_handler(query_text, \"formal\", \"bullets\", SUPPORTED_LANGS))  # âœ… run async safely in background\n",
        "      on_submit_handler(query_text, \"formal\", \"bullets\", SUPPORTED_LANGS)\n",
        "\n",
        "chat_box.on_submit(on_submit)\n",
        "#chat_box.observe(on_submit, names=\"value\")\n",
        "#chat_box.on_submit(lambda query: asyncio.ensure_future(on_submit_async(query, \"formal\", \"bullets\", SUPPORTED_LANGS)))\n",
        "display(widgets.VBox([chat_box, output_box]))"
      ],
      "metadata": {
        "id": "8HE8L3L7tYhI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588,
          "referenced_widgets": [
            "defabadf445b4e04af5257f446e65557",
            "36e77831eb804080a061ac41a91104a4",
            "196602831903449d91f0b803b7170403",
            "8e0883cf670741ceb7f84a4c38669006",
            "917d41b7d6f04e5da4fe66ff17606655",
            "ff40d37c81754933ab3a4655850b91b9",
            "d7f66973ea904135a0c9279595f7bd3a"
          ]
        },
        "outputId": "bf6a74c5-2ef5-49db-8a02-a57c24f373da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Text(value='', description='ğŸŒ¾ Farmer:', layout=Layout(width='80%'), placeholder='Ask about cropâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "defabadf445b4e04af5257f446e65557"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9Nvebgrm0rnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\"\"\"Why FAISS + multilingual embeddings? â†’ FAISS makes retrieval fast, and multilingual embeddings ensure good vector matches even for translated queries.\n",
        "  Async/await usage â†’ Lets you run translations and queries without blocking execution (good for scaling later).\"\"\"\n",
        "\n",
        "response = await multilingual_query_v1(\"à®¨à¯†à®²à¯ à®šà®¾à®•à¯à®ªà®Ÿà®¿à®•à¯à®•à¯ à®à®µà¯à®µà®³à®µà¯ à®®à®´à¯ˆ à®¤à¯‡à®µà¯ˆà®¯à®¾à®•à¯à®®à¯?\", retriever, llm,\"bullets\",\"formal\")\n",
        "print(\"Answer:\", response)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ru-x8GNmuVU4",
        "outputId": "1c75958f-f678-4bc2-9f1f-c5231e2ce35c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"\"\"Why FAISS + multilingual embeddings? â†’ FAISS makes retrieval fast, and multilingual embeddings ensure good vector matches even for translated queries.\\n  Async/await usage â†’ Lets you run translations and queries without blocking execution (good for scaling later).\"\"\"\\n\\nresponse = await multilingual_query_v1(\"à®¨à¯†à®²à¯ à®šà®¾à®•à¯à®ªà®Ÿà®¿à®•à¯à®•à¯ à®à®µà¯à®µà®³à®µà¯ à®®à®´à¯ˆ à®¤à¯‡à®µà¯ˆà®¯à®¾à®•à¯à®®à¯?\", retriever, llm,\"bullets\",\"formal\")\\nprint(\"Answer:\", response)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"à¤§à¤¾à¤¨ à¤•à¥€ à¤–à¥‡à¤¤à¥€ à¤•à¥‡ à¤²à¤¿à¤ à¤‰à¤ªà¤¯à¥à¤•à¥à¤¤ à¤œà¤²à¤µà¤¾à¤¯à¥ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\"\n",
        "\"à¦§à¦¾à¦¨ à¦šà¦¾à¦·à§‡à¦° à¦œà¦¨à§à¦¯ à¦‰à¦ªà¦¯à§à¦•à§à¦¤ à¦œà¦²à¦¬à¦¾à¦¯à¦¼à§ à¦•à§€?\"\n",
        "\"What is the suitable climate for paddy cultivation?\"\n",
        "\"à®¨à¯†à®²à¯ à®šà®¾à®•à¯à®ªà®Ÿà®¿à®•à¯à®•à¯ à®à®±à¯à®± à®•à®¾à®²à®¨à®¿à®²à¯ˆ à®à®¤à¯?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IX8m00Ei18PR",
        "outputId": "fead7253-36c2-427b-ee73-817e8db89d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'à®¨à¯†à®²à¯ à®šà®¾à®•à¯à®ªà®Ÿà®¿à®•à¯à®•à¯ à®à®±à¯à®± à®•à®¾à®²à®¨à®¿à®²à¯ˆ à®à®¤à¯?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''# ğŸ”¹ Step 7: Ask question and get multilingual answers\n",
        "query = \"What is the suitable climate for paddy cultivation?\"\n",
        "query1 = \"What is the suitable climate for paddy cultivation?\"\n",
        "query2 = \"Which type soil is best for paddy cultivation?\"\n",
        "query3 = \"Can you explain a bit more?\"\n",
        "query4 = \"Can you translate this answer to Hindi?\"\n",
        "response_en = qa_chain.invoke(query)\n",
        "print(\"\\nğŸ’¬ Answer:\", response_en)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "wq4o7r2E0pkP",
        "outputId": "18abdb32-aac8-48f4-df89-fcfb351a2acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# ğŸ”¹ Step 7: Ask question and get multilingual answers\\nquery = \"What is the suitable climate for paddy cultivation?\"\\nquery1 = \"What is the suitable climate for paddy cultivation?\"\\nquery2 = \"Which type soil is best for paddy cultivation?\"\\nquery3 = \"Can you explain a bit more?\"\\nquery4 = \"Can you translate this answer to Hindi?\"\\nresponse_en = qa_chain.invoke(query)\\nprint(\"\\nğŸ’¬ Answer:\", response_en)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "512-(50+128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1FB_1zLSS07",
        "outputId": "fbf22463-54e9-452b-e898-698915f51942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "334"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vars_dict = {\n",
        "      \"input\": user_query,\n",
        "      \"context\": trimmed_context,\n",
        "      \"tone\": tone,\n",
        "      \"user_lang\": user_lang,\n",
        "      \"style_instructions\": style_instructions,\n",
        "      \"chat_history\": \"\"\n",
        "      }\n",
        "prompt_prefix = qa_prompt_template.format(**vars_dict)\n",
        "prompt_tokens_est = n_tokens(prompt_prefix)\n",
        "MODEL_MAX_TOKENS = 512    # from the error\n",
        "ANSWER_TOKEN_BUDGET = 128\n",
        "allowed_context_tokens = MODEL_MAX_TOKENS - prompt_tokens_est - ANSWER_TOKEN_BUDGET\n",
        "if allowed_context_tokens < 50:\n",
        "  allowed_context_tokens = 50\n",
        "\n",
        "context = \" \".join([doc.page_content for doc in docs])\n",
        "trimmed_context = trim_text_by_tokens(context, allowed_context_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "_ooNmpodSVIT",
        "outputId": "6acd9f5a-447f-49e6-8fc1-154f2995756d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    You are an agricultural expert helping Indian farmers.\\nAnswer the question based on the context retrieved.\\nThe answer should be to the point and precise.\\nAvoid repeating facts in the response.\\nUse a formal tone.\\nAdhere to this answering format : \\nRespond in en language(same as user query).\\nAvoid hallucinating facts not present in the context.\\n\\nQuestion:\\ni love you\\n\\nContext:\\n\\n\\nChat History:\\n\\n\\nIf the answer is not in the context, reply: \"I could not find this information.\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rmfZLNgqaWG6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}